{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from google.colab import files\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV, KFold, cross_validate\n",
        "\n",
        "import scipy.stats as stats"
      ],
      "metadata": {
        "id": "f68WkfotxBD5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "Xae1qaG6GECw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezbtaiEarwOK"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/housing_price_dataset.csv\"\n",
        "\n",
        "dataframe = pd.read_csv(dataset_path)\n",
        "\n",
        "#Remocão de valores negativos no preco\n",
        "dataframe = dataframe[dataframe['Price'] >=0]\n",
        "\n",
        "print(dataframe.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 3: (NOVO) ENGENHARIA DE FEATURES\n",
        "# Criamos novas features a partir das existentes para tentar melhorar o modelo.\n",
        "# Esta etapa é feita ANTES de separar X e y.\n",
        "print(\"\\n--- Aplicando Engenharia de Features ---\")\n",
        "\n",
        "# 1. Criando a feature 'HouseAge' (Idade do Imóvel)\n",
        "current_year = 2025 # Usando o ano de referência da nossa conversa\n",
        "dataframe['HouseAge'] = current_year - dataframe['YearBuilt']\n",
        "\n",
        "# 2. Criando a feature 'BathToBedRatio' (Proporção de Banheiros por Quarto)\n",
        "dataframe['BathToBedRatio'] = dataframe['Bathrooms'] / (dataframe['Bedrooms'] + 1)\n",
        "\n",
        "# 3. Criando um novo DataFrame e removendo a coluna original 'YearBuilt'\n",
        "dataframe_eng = dataframe.drop(columns=['YearBuilt'])\n",
        "\n",
        "print(\"\\n--- DataFrame Após Engenharia de Features (Primeiras 5 Linhas) ---\")\n",
        "print(dataframe_eng.head())"
      ],
      "metadata": {
        "id": "GjmOqOjE3-th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ANÁLISE DOS DADOS\n",
        "\n",
        "# Vemos que não temos valores nulos no dataset, e que temos uma coluna categórica (Neighborhood)\n",
        "# com valores do tipo object, então terá que ser tratado na fase de pré processamento\n",
        "dataframe_eng.info()"
      ],
      "metadata": {
        "id": "e0rzejWxJxhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe_eng.describe()"
      ],
      "metadata": {
        "id": "d7HJkzfYKnCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRÉ-PROCESSAMENTO\n",
        "\n",
        "# Converter a coluna categórica para valores númericos\n",
        "# Escalonar as colunas\n",
        "\n",
        "# Separando features\n",
        "y = dataframe_eng['Price']\n",
        "\n",
        "X = dataframe_eng.drop(columns=['Price'])\n",
        "\n",
        "print('Variáveis independentes (Features)')\n",
        "print(X.head())\n",
        "\n",
        "print('Varíavel dependente (Alvo)')\n",
        "print(y.head())"
      ],
      "metadata": {
        "id": "htPSObdjTrdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot Enconding para converter os valores de Neighborhood\n",
        "# StandardScaler para escalonar as features\n",
        "# Vamos fazer isso usando o ColumnTransformer, para criar um \"pré-processador\"\n",
        "\n",
        "# Identificando as colunas\n",
        "categorical_features = ['Neighborhood']\n",
        "numerical_features = X.select_dtypes(include = np.number).columns.tolist()\n",
        "\n",
        "print(f'Colunas categóricas: {categorical_features}')\n",
        "print(f'Colunas para escalar: {numerical_features}')\n",
        "\n",
        "# Criando o pré-processador\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "      ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
        "      ('poly_num', Pipeline(steps = [('poly', PolynomialFeatures(degree = 2, include_bias = False)), ('scaler', StandardScaler())]), numerical_features),\n",
        "    ],\n",
        "    remainder = 'passthrough'\n",
        ")"
      ],
      "metadata": {
        "id": "iLZqAAgtdioc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DIVISÃO DE DADOS EM DADOS DE TREINAMENTO E TESTE\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print('Tamanho dos conjuntos de dados')\n",
        "print(f'X_train: {X_train.shape}')\n",
        "print(f'X_test: {X_test.shape}')\n",
        "print(f'y_train: {y_train.shape}')\n",
        "print(f'y_test: {y_test.shape}')"
      ],
      "metadata": {
        "id": "CJkzVWPnOc1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TREINAMENTO DOS MODELOS COM BUSCA POR HIPERPARÂMETROS E COM AS\n",
        "# MÉTRICAS DA VALIDAÇÃO CRUZADA (DEMORADO EM)\n",
        "\n",
        "models = {\n",
        "    'Regressão Linear': LinearRegression(),\n",
        "    'Lasso': Lasso(random_state=42, max_iter = 2000),\n",
        "    'SVR': SVR(),\n",
        "    'Random Forest': RandomForestRegressor(random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(random_state=42)\n",
        "}\n",
        "\n",
        "param_grids = {\n",
        "    'Lasso': {'regressor__alpha': np.logspace(-4, 1, 100)},\n",
        "    'SVR': {'regressor__kernel': ['rbf', 'linear'], 'regressor__C': [0.1, 1, 10, 100], 'regressor__gamma': ['scale', 'auto'], 'regressor__epsilon': [0.01, 0.1, 0.2]},\n",
        "    'Random Forest': {'regressor__n_estimators': [100, 200, 300], 'regressor__max_depth': [10, 20, 30, None], 'regressor__min_samples_leaf': [1, 2, 4]},\n",
        "    'Gradient Boosting': {'regressor__n_estimators': [100, 200, 300], 'regressor__learning_rate': [0.01, 0.05, 0.1], 'regressor__max_depth': [3, 5, 7]}\n",
        "}\n",
        "\n",
        "final_results = {}\n",
        "best_estimators = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nPROCESSANDO MODELO: {name}\")\n",
        "\n",
        "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', model)])\n",
        "\n",
        "    # Busca de Hiperparâmetros\n",
        "    if name in param_grids:\n",
        "        print(\"\\nBuscando hiperparâmetros...\")\n",
        "        random_search = RandomizedSearchCV(pipeline, param_distributions=param_grids[name], n_iter=20, cv=5, verbose=1, random_state=42, n_jobs=-1, scoring='r2')\n",
        "        random_search.fit(X_train, y_train)\n",
        "        best_model = random_search.best_estimator_\n",
        "        print(f\"Melhores parâmetros para {name}: {random_search.best_params_}\")\n",
        "    else:\n",
        "        best_model = pipeline\n",
        "\n",
        "    best_estimators[name] = best_model\n",
        "\n",
        "    # Avaliação\n",
        "    print(\"\\nIniciando avaliação robusta\")\n",
        "\n",
        "    list_of_r2s, list_of_maes, list_of_rmses = [], [], []\n",
        "    n_repeats = 30\n",
        "\n",
        "    scoring_metrics = ['r2', 'neg_mean_absolute_error', 'neg_root_mean_squared_error']\n",
        "\n",
        "    for i in range(n_repeats):\n",
        "        cv_strategy = KFold(n_splits=5, shuffle=True, random_state=i)\n",
        "\n",
        "        scores_dict = cross_validate(best_model, X_train, y_train, cv=cv_strategy, scoring=scoring_metrics, n_jobs=-1)\n",
        "\n",
        "        list_of_r2s.append(scores_dict['test_r2'].mean())\n",
        "        list_of_maes.append(-scores_dict['test_neg_mean_absolute_error'].mean())\n",
        "        list_of_rmses.append(-scores_dict['test_neg_root_mean_squared_error'].mean())\n",
        "\n",
        "    final_results[name] = {\n",
        "        'Média R²': np.mean(list_of_r2s),\n",
        "        'Std R²': np.std(list_of_r2s),\n",
        "        'Média MAE': np.mean(list_of_maes),\n",
        "        'Std MAE': np.std(list_of_maes),\n",
        "        'Média RMSE': np.mean(list_of_rmses),\n",
        "        'Std RMSE': np.std(list_of_rmses)\n",
        "    }\n",
        "\n",
        "    print(f\"Resultado final para {name}: R² = {final_results[name]['Média R²']:.4f}, MAE = {final_results[name]['Média MAE']:,.2f}, RMSE = {final_results[name]['Média RMSE']:,.2f}\")\n",
        "\n",
        "\n",
        "# RESULTADOS\n",
        "\n",
        "print(f\"\\nTABELA COMPARATIVA FINAL\")\n",
        "df_final_results = pd.DataFrame(final_results).T\n",
        "df_final_results = df_final_results.sort_values(by='Média R²', ascending=False)\n",
        "print(df_final_results)\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x=df_final_results.index, y=df_final_results['Média R²'])\n",
        "plt.errorbar(x=df_final_results.index, y=df_final_results['Média R²'], yerr=df_final_results['Std R²'], fmt='none', c='black', capsize=5)\n",
        "plt.title('Comparação Robusta do R² entre os Modelos (Média de 30x5-Fold CV)', fontsize=16)\n",
        "plt.xlabel('Modelo Otimizado', fontsize=12)\n",
        "plt.ylabel('R² (Coeficiente de Determinação)', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylim(0, max(df_final_results['Média R²']) * 1.1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nANÁLISE GRÁFICA DO MELHOR MODELO\")\n",
        "\n",
        "best_model_name = df_final_results.index[0]\n",
        "print(f\"O melhor modelo, com base no R² médio, foi: '{best_model_name}'\")\n",
        "\n",
        "final_model = best_estimators[best_model_name]\n",
        "\n",
        "# Retreinando o melhor modelo com todos os dados de treino para a previsão final\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_final = final_model.predict(X_test)\n",
        "\n",
        "# Gráfico de Previsões vs. Valores Reais\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(x=y_test, y=y_pred_final, alpha=0.5)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\n",
        "plt.title(f'Previsões vs. Valores Reais - {best_model_name}', fontsize=16)\n",
        "plt.xlabel('Preços Reais (y_test)', fontsize=12)\n",
        "plt.ylabel('Preços Previstos (y_pred)', fontsize=12)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Histograma dos Resíduos\n",
        "residuals = y_test - y_pred_final\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(residuals, kde=True, bins=50)\n",
        "plt.axvline(x=0, color='red', linestyle='--')\n",
        "plt.title(f'Distribuição dos Resíduos - {best_model_name}', fontsize=16)\n",
        "plt.xlabel('Resíduo (Preço Real - Preço Previsto)', fontsize=12)\n",
        "plt.ylabel('Frequência', fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nAnálise dos Resíduos:\")\n",
        "print(f\"  Média dos resíduos: R$ {residuals.mean():,.2f}\")"
      ],
      "metadata": {
        "id": "3NjYULIpJFej"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}